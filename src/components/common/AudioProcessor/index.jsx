import React, { useState, useRef } from "react";
import { Mic, LoaderCircle, CircleStop, Brain, Speech, Heart } from 'lucide-react';
import { Button } from "@/components/ui/button"
import { useResponseStore } from "@/stores/responseStore"

// ğŸ¤ éŒ²éŸ³ç”¨ã‚«ã‚¹ã‚¿ãƒ ãƒ•ãƒƒã‚¯
function useAudioRecorder(setParticleIntensity, setIsPlaying, setIsWaiting) {
  const [audioBlob, setAudioBlob] = useState(null);
  const [isRecording, setIsRecording] = useState(false);
  const mediaRecorder = useRef(null);
  const streamRef = useRef(null);
  let audioChunks = [];
  const audioRef = useRef(new Audio());
  const audioContextRef = useRef(null);
  const analyserRef = useRef(null);
  const dataArrayRef = useRef(new Uint8Array(128));
  const audioSourceRef = useRef(null);
  const intensityRef = useRef(0);

  // ã‚½ã‚±ãƒƒãƒˆç”¨
  const ws = useRef(null);
  const audioContext = useRef(null);
  const sourceBuffer = useRef(null);
  const { setResponseList, setRoleList } = useResponseStore();

  // ğŸ”´ éŒ²éŸ³é–‹å§‹
  const startRecording = async () => {
    console.log("Start Recording button clicked");
    try {
      setIsRecording(true);
      setIsPlaying(false);
      streamRef.current = await navigator.mediaDevices.getUserMedia({ audio: true });
      mediaRecorder.current = new MediaRecorder(streamRef.current, { mimeType: "audio/webm" });
      audioChunks = [];

      mediaRecorder.current.ondataavailable = (event) => {
        audioChunks.push(event.data);
      };

      mediaRecorder.current.onstop = () => {
        if (audioChunks.length > 0) {
          const audioBlob = new Blob(audioChunks, { type: "audio/webm" });
          setAudioBlob(audioBlob);
          console.log("Audio recorded successfully:", audioBlob);

          // ã‚½ã‚±ãƒƒãƒˆé€šä¿¡ãŒã§ãã‚Œã°ã„ã„
          ws.current = new WebSocket("ws://127.0.0.1:8000/ws");
          ws.current.binaryType = "arraybuffer"; // ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿å—ä¿¡è¨­å®š

          ws.current.onopen = async () => {
            console.log("WebSocket æ¥ç¶šæˆåŠŸï¼");
            const arrayBuffer = await audioBlob.arrayBuffer();
            console.log("é€ä¿¡ã™ã‚‹ãƒ‡ãƒ¼ã‚¿:", new Uint8Array(arrayBuffer).slice(0, 20)); // æœ€åˆã® 20 ãƒã‚¤ãƒˆã‚’è¡¨ç¤º
            ws.current.send(arrayBuffer); // æœ€åˆã« "mp3" ã¾ãŸã¯ "json" ã‚’é€ä¿¡
          };

          ws.current.onmessage = async (event) => {
            if (typeof event.data === "string") {
              const response = JSON.parse(event.data);
              // JSON ã®å ´åˆ
              console.log("JSON ãƒ‡ãƒ¼ã‚¿å—ä¿¡:", response);

              // å—ä¿¡ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ãƒªã‚¹ãƒˆã«åæ˜ 
              // @todo: setLanguage, animateSetTextã‚’ã™ã‚‹ã¨éŸ³å£°å—ä¿¡æ™‚ã«ã‚ªãƒ¬ãƒ³ã‚¸ã«å…‰ã‚‰ãªããªã‚‹
              if (response.type === 0) {
                // ãƒˆãƒ¼ã‚¯ã«è¿½åŠ 
                setResponseList(response)
              } else if (response.type === 1) {
                // è¨­å®šãƒ­ã‚°ã«è¿½åŠ 
                const data = {...response}
                switch (response.role.type ) {
                  case 0:
                    data.src = 'https://github.com/ShinjoSato.png'
                    data.batchIcon = <Brain className="h-4 w-4 text-white" />
                    break
                  case 1:
                    data.src = 'https://github.com/ShinjoSato.png'
                    data.batchIcon = <Speech className="h-4 w-4 text-white" />
                    break
                  case 2:
                    data.src = '/dummy_hattori.png'
                    data.batchIcon = <Heart className="h-4 w-4 text-white" />
                    break
                }
                setRoleList(data)
              }
            } else if (event.data instanceof ArrayBuffer) {
                // MP3 ã®å ´åˆ
                console.log("MP3 ãƒ‡ãƒ¼ã‚¿å—ä¿¡");

                if (!audioContext.current) {
                    audioContext.current = new AudioContext();
                }
                const arrayBuffer = event.data;
                const audioBuffer = await audioContext.current.decodeAudioData(arrayBuffer);
                const newSource = audioContext.current.createBufferSource();
                newSource.buffer = audioBuffer;
                newSource.connect(audioContext.current.destination);
                newSource.onended = () => {
                  setIsPlaying(false);
                  intensityRef.current = 0;
                  setParticleIntensity(0);
                  setIsWaiting(false);
                }
                newSource.start();
                setIsPlaying(true);
            }
          };

          ws.current.onclose = () => {
              console.log("WebSocket åˆ‡æ–­");
          };

          // ws.current.close();


          
          // // ğŸ”Š è‡ªå‹•å†ç”Ÿ
          // const audioUrl = URL.createObjectURL(audioBlob);
          // audioRef.current.src = audioUrl;
          // audioRef.current.onended = () => {
          //   setIsPlaying(false);
          //   intensityRef.current = 0;
          //   setParticleIntensity(0);
          // }; // å†ç”Ÿçµ‚äº†æ™‚ã«ãƒ•ãƒ©ã‚°ã¨ç²’å­åŠ¹æœã‚’ãƒªã‚»ãƒƒãƒˆ

          // // å†ç”Ÿé–‹å§‹
          // audioRef.current.play().catch((error) => console.error("Playback error:", error));
          // setIsPlaying(true);
          // analyzeAudio(audioRef.current);
        }
      };

      mediaRecorder.current.start();
      console.log("Recording started...");
      analyzeLiveAudio(streamRef.current);
    } catch (error) {
      console.error("Error accessing microphone:", error);
    }
  };

  // ğŸµ ãƒ©ã‚¤ãƒ–éŒ²éŸ³ä¸­ã®éŸ³å£°è§£æ
  const analyzeLiveAudio = (stream) => {
    if (!audioContextRef.current) {
      audioContextRef.current = new (window.AudioContext || window.webkitAudioContext)();
    }
    const source = audioContextRef.current.createMediaStreamSource(stream);
    analyserRef.current = audioContextRef.current.createAnalyser();
    analyserRef.current.fftSize = 256;
    source.connect(analyserRef.current);

    const updateAudioLevel = () => {
      analyserRef.current.getByteFrequencyData(dataArrayRef.current);
      const maxLevel = Math.max(...dataArrayRef.current);
      const normalizedLevel = maxLevel / 255;
      intensityRef.current = normalizedLevel;
      setParticleIntensity(normalizedLevel);
      requestAnimationFrame(updateAudioLevel);
    };
    updateAudioLevel();
  };

  // ğŸµ å†ç”Ÿä¸­ã®éŸ³å£°è§£æ
  const analyzeAudio = (audioElement) => {
    if (!audioContextRef.current) {
      audioContextRef.current = new (window.AudioContext || window.webkitAudioContext)();
    }
    try {
      // @todo: å†ç”Ÿã«åˆã‚ã›ã¦ç²’å­ãŒæ³¢æ‰“ã¤ãŒã€äºŒå›ç›®ä»¥é™ã§ã‚¨ãƒ©ãƒ¼
      audioSourceRef.current = audioContextRef.current.createMediaElementSource(audioElement);
      analyserRef.current = audioContextRef.current.createAnalyser();
      analyserRef.current.fftSize = 256;
      audioSourceRef.current.connect(analyserRef.current);
      analyserRef.current.connect(audioContextRef.current.destination);
    } catch (error) {
      console.error("Failed to create MediaElementSource:", error);
      return;
    }

    const updateAudioLevel = () => {
      if (analyserRef.current) {
        analyserRef.current.getByteFrequencyData(dataArrayRef.current);
        const maxLevel = Math.max(...dataArrayRef.current);
        const normalizedLevel = maxLevel / 255;
        intensityRef.current = normalizedLevel;
        setParticleIntensity(normalizedLevel);
        requestAnimationFrame(updateAudioLevel);
      }
    };
    updateAudioLevel();
  };

  // ğŸ›‘ éŒ²éŸ³åœæ­¢
  const stopRecording = () => {
    console.log("Stop Recording button clicked");
    if (mediaRecorder.current && isRecording) {
      mediaRecorder.current.stop();
      streamRef.current.getTracks().forEach(track => track.stop());
      console.log("Recording stopped...");
      setIsRecording(false);
      setIsPlaying(false);
      setIsWaiting(true);
      intensityRef.current = 0;
      setParticleIntensity(0);
    } else {
      console.error("MediaRecorder is not initialized or already stopped");
    }
  };

  return { audioBlob, isRecording, startRecording, stopRecording };
}

// ğŸ› éŒ²éŸ³ãƒœã‚¿ãƒ³ã‚’è¡¨ç¤ºã™ã‚‹ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
export default function AudioProcessor({ setParticleIntensity, setIsPlaying }) {
  const [isWaiting, setIsWaiting] = useState(false)
  const { isRecording, startRecording, stopRecording } = useAudioRecorder(setParticleIntensity, setIsPlaying, setIsWaiting);

  return (
    <div className="text-center">
      <Button
        className="group p-[10px] text-white group-hover:text-black"
        variant="ghost"
        size="icon"
        onClick={isRecording ? () => stopRecording() : () => startRecording()}
        disabled={isWaiting}
      >
        {isRecording ? (
          <CircleStop />
        ) : isWaiting ? (
          <LoaderCircle  className="spin" />
        ) : (
          <Mic />
        )}
      </Button>
    </div>
  );
}
